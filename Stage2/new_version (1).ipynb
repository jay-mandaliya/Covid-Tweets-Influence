{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-b3d1071a5684>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  truncated_covid['new_recovered'] = truncated_covid['new_recovered'].fillna(0)\n",
      "<ipython-input-3-b3d1071a5684>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  truncated_covid['cumulative_recovered'] = truncated_covid['cumulative_recovered'].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_covid = pd.read_csv('./data/covid.csv', dtype={'datacommons_id': str})\n",
    "\n",
    "selected_fields = ['date', 'new_confirmed', 'new_recovered', 'cumulative_confirmed', 'cumulative_recovered']\n",
    "selected_covid = raw_covid[selected_fields]\n",
    "\n",
    "truncated_covid = selected_covid.head(283);\n",
    "\n",
    "truncated_covid['new_recovered'] = truncated_covid['new_recovered'].fillna(0)\n",
    "truncated_covid['cumulative_recovered'] = truncated_covid['cumulative_recovered'].fillna(0)\n",
    "\n",
    "truncated_covid.to_csv('./Stage2/covid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-fb08c914852e>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_govcanhealth['date'] = pd.to_datetime(selected_govcanhealth['date'])\n",
      "<ipython-input-5-fb08c914852e>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_cdcgov['date'] = pd.to_datetime(selected_cdcgov['date'])\n",
      "<ipython-input-5-fb08c914852e>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_cbcnews['date'] = pd.to_datetime(selected_cbcnews['date'])\n",
      "<ipython-input-5-fb08c914852e>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_globalnews['date'] = pd.to_datetime(selected_globalnews['date'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_govcanhealth = pd.read_csv('./data/govcanhealth.csv')\n",
    "raw_cdcgov = pd.read_csv('./data/cdcgov.csv')\n",
    "raw_cbcnews = pd.read_csv('./data/cbcnews.csv')\n",
    "raw_globalnews = pd.read_csv('./data/globalnews.csv')\n",
    "\n",
    "selected_fields = ['date', 'tweet', 'hashtags',]\n",
    "selected_govcanhealth = raw_govcanhealth[selected_fields]\n",
    "selected_cdcgov = raw_cdcgov[selected_fields]\n",
    "selected_cbcnews = raw_cbcnews[selected_fields]\n",
    "selected_globalnews = raw_globalnews[selected_fields]\n",
    "\n",
    "selected_govcanhealth['date'] = pd.to_datetime(selected_govcanhealth['date'])\n",
    "selected_cdcgov['date'] = pd.to_datetime(selected_cdcgov['date'])\n",
    "selected_cbcnews['date'] = pd.to_datetime(selected_cbcnews['date'])\n",
    "selected_globalnews['date'] = pd.to_datetime(selected_globalnews['date'])\n",
    "\n",
    "start_date = '11-01-2019'\n",
    "end_date = '10-09-2020'\n",
    "mask_govcanhealth = (selected_govcanhealth['date'] >= start_date) & (selected_govcanhealth['date'] <= end_date)\n",
    "mask_cdcgov = (selected_cdcgov['date'] >= start_date) & (selected_cdcgov['date'] <= end_date)\n",
    "mask_cbcnews = (selected_cbcnews['date'] >= start_date) & (selected_cbcnews['date'] <= end_date)\n",
    "mask_globalnews = (selected_globalnews['date'] >= start_date) & (selected_globalnews['date'] <= end_date)\n",
    "\n",
    "truncated_govcanhealth = selected_govcanhealth.loc[mask_govcanhealth]\n",
    "truncated_cdcgov = selected_cdcgov.loc[mask_cdcgov]\n",
    "truncated_cbcnews = selected_cbcnews.loc[mask_cbcnews]\n",
    "truncated_globalnews = selected_globalnews.loc[mask_globalnews]\n",
    "\n",
    "truncated_govcanhealth.to_csv('./Stage2/govcanhealth.csv', index=False)\n",
    "truncated_cdcgov.to_csv('./Stage2/cdcgov.csv', index=False)\n",
    "truncated_cbcnews.to_csv('./Stage2/cbcnews.csv', index=False)\n",
    "truncated_globalnews.to_csv('./Stage2/globalnews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_govcanhealth = pd.read_csv('./Stage2/govcanhealth.csv')\n",
    "raw_cdcgov = pd.read_csv('./Stage2/cdcgov.csv')\n",
    "raw_cbcnews = pd.read_csv('./Stage2/cbcnews.csv')\n",
    "raw_globalnews = pd.read_csv('./Stage2/globalnews.csv')\n",
    "\n",
    "keywords = ['covid', 'physicaldistancing', 'publichealth', 'coronavirus', 'pandemic', 'mask']\n",
    "pattern = '|'.join(keywords)\n",
    "\n",
    "filtered_govcanhealth = raw_govcanhealth[raw_govcanhealth['tweet'].str.contains(pattern, case=False)]\n",
    "filtered_cdcgov = raw_cdcgov[raw_cdcgov['tweet'].str.contains(pattern, case=False)]\n",
    "filtered_cbcnews = raw_cbcnews[raw_cbcnews['tweet'].str.contains(pattern, case=False)]\n",
    "filtered_globalnews = raw_globalnews[raw_globalnews['tweet'].str.contains(pattern, case=False)]\n",
    "\n",
    "filtered_govcanhealth.to_csv('./Stage3/govcanhealth.csv', index=False)\n",
    "filtered_cdcgov.to_csv('./Stage3/cdcgov.csv', index=False)\n",
    "filtered_cbcnews.to_csv('./Stage3/cbcnews.csv', index=False)\n",
    "filtered_globalnews.to_csv('./Stage3/globalnews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date  new_confirmed  new_recovered  cumulative_confirmed  \\\n",
      "0   2020-01-01            0.0            0.0                   0.0   \n",
      "1   2020-01-02            0.0            0.0                   0.0   \n",
      "2   2020-01-03            0.0            0.0                   0.0   \n",
      "3   2020-01-04            0.0            0.0                   0.0   \n",
      "4   2020-01-05            0.0            0.0                   0.0   \n",
      "..         ...            ...            ...                   ...   \n",
      "277 2020-10-04         1685.0         1376.0              166156.0   \n",
      "278 2020-10-05         2804.0         2091.0              168960.0   \n",
      "279 2020-10-06         2363.0         1660.0              171323.0   \n",
      "280 2020-10-07         1800.0         1672.0              173123.0   \n",
      "281 2020-10-08         2436.0         1842.0              175559.0   \n",
      "\n",
      "     cumulative_recovered  \n",
      "0                     0.0  \n",
      "1                     0.0  \n",
      "2                     0.0  \n",
      "3                     0.0  \n",
      "4                     0.0  \n",
      "..                    ...  \n",
      "277              140243.0  \n",
      "278              142334.0  \n",
      "279              143994.0  \n",
      "280              145666.0  \n",
      "281              147508.0  \n",
      "\n",
      "[282 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "covid = pd.read_csv('./Stage2/covid.csv')\n",
    "govcanhealth = pd.read_csv('./Stage3/govcanhealth.csv')\n",
    "cdcgov = pd.read_csv('./Stage3/cdcgov.csv')\n",
    "cbcnews = pd.read_csv('./Stage3/cbcnews.csv')\n",
    "globalnews = pd.read_csv('./Stage3/globalnews.csv')\n",
    "\n",
    "covid['date'] = pd.to_datetime(covid['date'])\n",
    "govcanhealth['date'] = pd.to_datetime(govcanhealth['date'])\n",
    "cdcgov['date'] = pd.to_datetime(cdcgov['date'])\n",
    "cbcnews['date'] = pd.to_datetime(cbcnews['date'])\n",
    "globalnews['date'] = pd.to_datetime(globalnews['date'])\n",
    "\n",
    "start_date = '01-01-2020'\n",
    "end_date = '10-08-2020'\n",
    "mask_govcanhealth = (govcanhealth['date'] >= start_date) & (govcanhealth['date'] <= end_date)\n",
    "mask_cdcgov = (cdcgov['date'] >= start_date) & (cdcgov['date'] <= end_date)\n",
    "mask_cbcnews = (cbcnews['date'] >= start_date) & (cbcnews['date'] <= end_date)\n",
    "mask_globalnews = (globalnews['date'] >= start_date) & (globalnews['date'] <= end_date)\n",
    "mask_covid = (covid['date'] >= start_date) & (covid['date'] <= end_date)\n",
    "govcanhealth = govcanhealth.loc[mask_govcanhealth]\n",
    "cdcgov = cdcgov.loc[mask_cdcgov]\n",
    "cbcnews = cbcnews.loc[mask_cbcnews]\n",
    "globalnews = globalnews.loc[mask_globalnews]\n",
    "covid1 = covid.loc[mask_covid]\n",
    "column_names = ['date', 'tweet', 'hashtags', 'Changes', 'account']\n",
    "column_Covidnames = ['date','Changes']\n",
    "data = pd.DataFrame(columns=column_names)\n",
    "coviddata = pd.DataFrame(columns=column_Covidnames)\n",
    "counter = 0\n",
    "CovidDay= 0\n",
    "\n",
    "print(covid1)\n",
    "\n",
    "for index, row in covid1.iterrows():\n",
    "    next_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date'] + timedelta(days=1)].values[0]\n",
    "    same_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date']].values[0]\n",
    "\n",
    "    coviddata.loc[CovidDay] = row\n",
    "    coviddata.loc[CovidDay, 'Changes'] = next_day_cases - same_day_cases\n",
    "\n",
    "    CovidDay = CovidDay + 1\n",
    "\n",
    "for index, row in govcanhealth.iterrows():\n",
    "    next_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date'] + timedelta(days=1)].values[0]\n",
    "    same_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date']].values[0]\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'govcanhealth'\n",
    "    data.loc[counter, 'Changes'] = next_day_cases - same_day_cases\n",
    "    counter = counter + 1\n",
    "\n",
    "\n",
    "for index, row in cdcgov.iterrows():\n",
    "    next_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date'] + timedelta(days=1)].values[0]\n",
    "    same_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date']].values[0]\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'cdcgov'\n",
    "    data.loc[counter, 'Changes'] = next_day_cases - same_day_cases\n",
    "\n",
    "    counter = counter + 1\n",
    "\n",
    "for index, row in cbcnews.iterrows():\n",
    "    next_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date'] + timedelta(days=1)].values[0]\n",
    "    same_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date']].values[0]\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'cbcnews'\n",
    "\n",
    "    data.loc[counter, 'Changes'] = next_day_cases - same_day_cases\n",
    "\n",
    "    counter = counter + 1\n",
    "\n",
    "for index, row in globalnews.iterrows():\n",
    "    next_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date'] + timedelta(days=1)].values[0]\n",
    "    same_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date']].values[0]\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'globalnews'\n",
    "\n",
    "    data.loc[counter, 'Changes'] = next_day_cases - same_day_cases\n",
    "\n",
    "    counter = counter + 1\n",
    "\n",
    "data.to_csv('./Dataset/data1day.csv', index=False)\n",
    "coviddata.to_csv('./Dataset/covid1day.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "covid = pd.read_csv('./Stage2/covid.csv')\n",
    "govcanhealth = pd.read_csv('./Stage3/govcanhealth.csv')\n",
    "cdcgov = pd.read_csv('./Stage3/cdcgov.csv')\n",
    "cbcnews = pd.read_csv('./Stage3/cbcnews.csv')\n",
    "globalnews = pd.read_csv('./Stage3/globalnews.csv')\n",
    "\n",
    "\n",
    "covid['date'] = pd.to_datetime(covid['date'])\n",
    "govcanhealth['date'] = pd.to_datetime(govcanhealth['date'])\n",
    "cdcgov['date'] = pd.to_datetime(cdcgov['date'])\n",
    "cbcnews['date'] = pd.to_datetime(cbcnews['date'])\n",
    "globalnews['date'] = pd.to_datetime(globalnews['date'])\n",
    "\n",
    "start_date = '01-15-2020'\n",
    "end_date = '09-25-2020'\n",
    "mask_govcanhealth = (govcanhealth['date'] >= start_date) & (govcanhealth['date'] <= end_date)\n",
    "mask_cdcgov = (cdcgov['date'] >= start_date) & (cdcgov['date'] <= end_date)\n",
    "mask_cbcnews = (cbcnews['date'] >= start_date) & (cbcnews['date'] <= end_date)\n",
    "mask_globalnews = (globalnews['date'] >= start_date) & (globalnews['date'] <= end_date)\n",
    "covid_mask = (covid['date'] >= start_date) & (covid['date'] <= end_date)\n",
    "govcanhealth = govcanhealth.loc[mask_govcanhealth]\n",
    "cdcgov = cdcgov.loc[mask_cdcgov]\n",
    "cbcnews = cbcnews.loc[mask_cbcnews]\n",
    "globalnews = globalnews.loc[mask_globalnews]\n",
    "covid1 = covid.loc[covid_mask]\n",
    "column_names = ['date', 'tweet', 'hashtags', 'Changes', 'account']\n",
    "column_Covidnames = ['date','Changes']\n",
    "data = pd.DataFrame(columns=column_names)\n",
    "coviddata = pd.DataFrame(columns=column_Covidnames)\n",
    "counter = 0\n",
    "CovidDay= 0\n",
    "\n",
    "\n",
    "\n",
    "def next_14_cases(date):\n",
    "    addition = 0\n",
    "    for i in range(14):\n",
    "        addition = addition + covid['new_confirmed'].loc[covid['date'] == date + timedelta(days=i+1)].values[0]\n",
    "\n",
    "    return addition\n",
    "\n",
    "\n",
    "def previous_14_cases(date):\n",
    "    addition = 0\n",
    "    for i in range(14):\n",
    "        addition = addition + covid['new_confirmed'].loc[covid['date'] == date - timedelta(days=i)].values[0]\n",
    "\n",
    "    return addition\n",
    "\n",
    "\n",
    "for index, row in covid1.iterrows():\n",
    "    next_cases = next_14_cases(row['date'])\n",
    "    previous_cases = previous_14_cases(row['date'])\n",
    "    coviddata.loc[CovidDay] = row\n",
    "    coviddata.loc[CovidDay, 'Changes'] = next_cases - previous_cases\n",
    "\n",
    "    CovidDay = CovidDay + 1\n",
    "\n",
    "for index, row in govcanhealth.iterrows():\n",
    "    next_cases = next_14_cases(row['date'])\n",
    "    previous_cases = previous_14_cases(row['date'])\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'govcanhealth'\n",
    "\n",
    "    data.loc[counter, 'Changes'] = next_cases - previous_cases\n",
    "\n",
    "    counter = counter + 1\n",
    "\n",
    "\n",
    "for index, row in cdcgov.iterrows():\n",
    "    next_cases = next_14_cases(row['date'])\n",
    "    previous_cases = previous_14_cases(row['date'])\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'cdcgov'\n",
    "\n",
    "    data.loc[counter, 'Changes'] = next_cases - previous_cases\n",
    "\n",
    "    counter = counter + 1\n",
    "\n",
    "for index, row in cbcnews.iterrows():\n",
    "    next_cases = next_14_cases(row['date'])\n",
    "    previous_cases = previous_14_cases(row['date'])\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'cbcnews'\n",
    "\n",
    "    data.loc[counter, 'Changes'] = next_cases - previous_cases\n",
    "\n",
    "    counter = counter + 1\n",
    "\n",
    "for index, row in globalnews.iterrows():\n",
    "    next_cases = next_14_cases(row['date'])\n",
    "    previous_cases = previous_14_cases(row['date'])\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'globalnews'\n",
    "\n",
    "    data.loc[counter, 'Changes'] = next_cases - previous_cases\n",
    "\n",
    "    counter = counter + 1\n",
    "\n",
    "data.to_csv('./Dataset/data14day.csv', index=False)\n",
    "\n",
    "coviddata.to_csv('./Dataset/Covid14day.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Changes</th>\n",
       "      <th>NormalChange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.356059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.356059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.356258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.356258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.356258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>2020-09-21</td>\n",
       "      <td>10272.0</td>\n",
       "      <td>0.866876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>11745.0</td>\n",
       "      <td>0.940126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>2020-09-23</td>\n",
       "      <td>11911.0</td>\n",
       "      <td>0.948381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>12295.0</td>\n",
       "      <td>0.967477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>2020-09-25</td>\n",
       "      <td>12831.0</td>\n",
       "      <td>0.994132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  Changes  NormalChange\n",
       "0    2020-01-15      0.0      0.356059\n",
       "1    2020-01-16      0.0      0.356059\n",
       "2    2020-01-17      4.0      0.356258\n",
       "3    2020-01-18      4.0      0.356258\n",
       "4    2020-01-19      4.0      0.356258\n",
       "..          ...      ...           ...\n",
       "250  2020-09-21  10272.0      0.866876\n",
       "251  2020-09-22  11745.0      0.940126\n",
       "252  2020-09-23  11911.0      0.948381\n",
       "253  2020-09-24  12295.0      0.967477\n",
       "254  2020-09-25  12831.0      0.994132\n",
       "\n",
       "[255 rows x 3 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans14 = KMeans(n_clusters=3, random_state=0)\n",
    "file = 'Dataset/covid1day.csv'\n",
    "file14 = 'Dataset/covid14day.csv'\n",
    "data_df14 = pd.read_csv(file14)\n",
    "data_df = pd.read_csv(file)\n",
    "data_Change14 = data_df14['Changes']\n",
    "data_Change = data_df['Changes']\n",
    "scaler14 = MinMaxScaler()\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_Change.to_numpy().reshape(-1,1))\n",
    "scaler14.fit(data_Change14.to_numpy().reshape(-1,1))\n",
    "normalized_Change = scaler.transform(data_Change.to_numpy().reshape(-1,1))\n",
    "normalized_Change14 = scaler14.transform(data_Change14.to_numpy().reshape(-1,1))\n",
    "data_df14['NormalChange'] =normalized_Change14\n",
    "data_df['NormalChange'] =normalized_Change\n",
    "\n",
    "data_df14\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter \n",
    "preds = kmeans.fit_predict(data_df['NormalChange'].to_numpy().reshape(-1,1))\n",
    "preds14 = kmeans14.fit_predict(data_df14['NormalChange'].to_numpy().reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51967387]), 0),\n",
       " (array([0.516838]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51931939]), 0),\n",
       " (array([0.51719248]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51861042]), 0),\n",
       " (array([0.51790145]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51861042]), 0),\n",
       " (array([0.51790145]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51861042]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51790145]), 0),\n",
       " (array([0.51896491]), 0),\n",
       " (array([0.52073733]), 0),\n",
       " (array([0.51506558]), 0),\n",
       " (array([0.5214463]), 0),\n",
       " (array([0.51506558]), 0),\n",
       " (array([0.52250975]), 0),\n",
       " (array([0.51612903]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51790145]), 0),\n",
       " (array([0.52180078]), 0),\n",
       " (array([0.51293867]), 0),\n",
       " (array([0.52747253]), 0),\n",
       " (array([0.52250975]), 0),\n",
       " (array([0.51825594]), 0),\n",
       " (array([0.51187522]), 0),\n",
       " (array([0.5303084]), 0),\n",
       " (array([0.5303084]), 0),\n",
       " (array([0.52215526]), 0),\n",
       " (array([0.53881602]), 0),\n",
       " (array([0.56043956]), 0),\n",
       " (array([0.46685572]), 0),\n",
       " (array([0.60191421]), 0),\n",
       " (array([0.42360865]), 2),\n",
       " (array([0.70258773]), 1),\n",
       " (array([0.54696916]), 0),\n",
       " (array([0.48847926]), 0),\n",
       " (array([0.52428217]), 0),\n",
       " (array([0.52250975]), 0),\n",
       " (array([0.55015952]), 0),\n",
       " (array([0.55264091]), 0),\n",
       " (array([0.64090748]), 1),\n",
       " (array([0.49415101]), 0),\n",
       " (array([0.50194966]), 0),\n",
       " (array([0.73271889]), 1),\n",
       " (array([0.3707905]), 2),\n",
       " (array([0.55831266]), 0),\n",
       " (array([0.60368664]), 0),\n",
       " (array([0.35767458]), 2),\n",
       " (array([0.54484225]), 0),\n",
       " (array([0.57568238]), 0),\n",
       " (array([0.54803261]), 0),\n",
       " (array([0.4852889]), 0),\n",
       " (array([0.4427508]), 0),\n",
       " (array([0.48103509]), 0),\n",
       " (array([0.60049628]), 0),\n",
       " (array([0.54874158]), 0),\n",
       " (array([0.49521446]), 0),\n",
       " (array([0.65756824]), 1),\n",
       " (array([0.54696916]), 0),\n",
       " (array([0.40411202]), 2),\n",
       " (array([0.50478554]), 0),\n",
       " (array([0.73555477]), 1),\n",
       " (array([0.3573201]), 2),\n",
       " (array([0.58099965]), 0),\n",
       " (array([0.57213754]), 0),\n",
       " (array([0.46791918]), 0),\n",
       " (array([0.40765686]), 2),\n",
       " (array([0.54484225]), 0),\n",
       " (array([0.54094293]), 0),\n",
       " (array([0.49025168]), 0),\n",
       " (array([0.53420773]), 0),\n",
       " (array([0.54236086]), 0),\n",
       " (array([0.58419]), 0),\n",
       " (array([0.45728465]), 0),\n",
       " (array([0.91066998]), 1),\n",
       " (array([0.]), 2),\n",
       " (array([0.50974832]), 0),\n",
       " (array([0.58064516]), 0),\n",
       " (array([0.50974832]), 0),\n",
       " (array([0.54874158]), 0),\n",
       " (array([0.43176179]), 2),\n",
       " (array([0.47500886]), 0),\n",
       " (array([0.51364764]), 0),\n",
       " (array([0.53349876]), 0),\n",
       " (array([0.49875931]), 0),\n",
       " (array([0.54980503]), 0),\n",
       " (array([0.48812478]), 0),\n",
       " (array([0.56292095]), 0),\n",
       " (array([0.47819922]), 0),\n",
       " (array([0.49415101]), 0),\n",
       " (array([0.50762141]), 0),\n",
       " (array([0.5079759]), 0),\n",
       " (array([0.58560794]), 0),\n",
       " (array([0.50230415]), 0),\n",
       " (array([0.51293867]), 0),\n",
       " (array([0.49592343]), 0),\n",
       " (array([0.49485998]), 0),\n",
       " (array([0.49131514]), 0),\n",
       " (array([0.49556895]), 0),\n",
       " (array([0.56114853]), 0),\n",
       " (array([0.48741581]), 0),\n",
       " (array([0.47075505]), 0),\n",
       " (array([0.51293867]), 0),\n",
       " (array([0.51861042]), 0),\n",
       " (array([0.49946827]), 0),\n",
       " (array([0.50762141]), 0),\n",
       " (array([0.50620347]), 0),\n",
       " (array([0.50691244]), 0),\n",
       " (array([0.55831266]), 0),\n",
       " (array([0.4898972]), 0),\n",
       " (array([0.48387097]), 0),\n",
       " (array([0.47004608]), 0),\n",
       " (array([0.54058844]), 0),\n",
       " (array([0.49450549]), 0),\n",
       " (array([0.52109181]), 0),\n",
       " (array([0.53739809]), 0),\n",
       " (array([0.48635236]), 0),\n",
       " (array([0.51222971]), 0),\n",
       " (array([0.50407657]), 0),\n",
       " (array([0.5416519]), 0),\n",
       " (array([0.51152074]), 0),\n",
       " (array([0.53314428]), 0),\n",
       " (array([0.51152074]), 0),\n",
       " (array([0.49273307]), 0),\n",
       " (array([0.51187522]), 0),\n",
       " (array([0.52747253]), 0),\n",
       " (array([0.50159518]), 0),\n",
       " (array([0.55405884]), 0),\n",
       " (array([0.44452322]), 0),\n",
       " (array([0.5416519]), 0),\n",
       " (array([0.51116625]), 0),\n",
       " (array([0.67777384]), 1),\n",
       " (array([0.38284296]), 2),\n",
       " (array([0.41687345]), 2),\n",
       " (array([0.71924849]), 1),\n",
       " (array([0.43034385]), 2),\n",
       " (array([0.4852889]), 0),\n",
       " (array([0.51577455]), 0),\n",
       " (array([0.5820631]), 0),\n",
       " (array([0.45905707]), 0),\n",
       " (array([0.53066289]), 0),\n",
       " (array([0.5551223]), 0),\n",
       " (array([0.50053173]), 0),\n",
       " (array([0.48280752]), 0),\n",
       " (array([0.52640907]), 0),\n",
       " (array([0.63204537]), 1),\n",
       " (array([0.43530663]), 2),\n",
       " (array([0.52180078]), 0),\n",
       " (array([0.55228642]), 0),\n",
       " (array([0.50691244]), 0),\n",
       " (array([0.49166962]), 0),\n",
       " (array([0.5214463]), 0),\n",
       " (array([0.67671039]), 1),\n",
       " (array([0.4427508]), 0),\n",
       " (array([0.50762141]), 0),\n",
       " (array([0.47890819]), 0),\n",
       " (array([0.55441333]), 0),\n",
       " (array([0.45303084]), 0),\n",
       " (array([0.52002836]), 0),\n",
       " (array([0.63559022]), 1),\n",
       " (array([0.41581]), 2),\n",
       " (array([0.5235732]), 0),\n",
       " (array([0.51152074]), 0),\n",
       " (array([0.56079404]), 0),\n",
       " (array([0.4381425]), 2),\n",
       " (array([0.51754697]), 0),\n",
       " (array([0.46933711]), 0),\n",
       " (array([0.73590925]), 1),\n",
       " (array([0.38851471]), 2),\n",
       " (array([0.51081177]), 0),\n",
       " (array([0.53598015]), 0),\n",
       " (array([0.4516129]), 0),\n",
       " (array([0.51612903]), 0),\n",
       " (array([0.67812832]), 1),\n",
       " (array([0.37929812]), 2),\n",
       " (array([0.56575682]), 0),\n",
       " (array([0.50655796]), 0),\n",
       " (array([0.5281815]), 0),\n",
       " (array([0.45409429]), 0),\n",
       " (array([0.50443105]), 0),\n",
       " (array([0.72633818]), 1),\n",
       " (array([0.33995037]), 2),\n",
       " (array([0.53739809]), 0),\n",
       " (array([0.5349167]), 0),\n",
       " (array([0.55937611]), 0),\n",
       " (array([0.43247076]), 2),\n",
       " (array([0.52180078]), 0),\n",
       " (array([0.6898263]), 1),\n",
       " (array([0.3661822]), 2),\n",
       " (array([0.56292095]), 0),\n",
       " (array([0.50159518]), 0),\n",
       " (array([0.550514]), 0),\n",
       " (array([0.47252747]), 0),\n",
       " (array([0.48422545]), 0),\n",
       " (array([0.78092875]), 1),\n",
       " (array([0.33002481]), 2),\n",
       " (array([0.52570011]), 0),\n",
       " (array([0.5437788]), 0),\n",
       " (array([0.53987948]), 0),\n",
       " (array([0.42609004]), 2),\n",
       " (array([0.52853598]), 0),\n",
       " (array([0.46401985]), 0),\n",
       " (array([1.]), 1),\n",
       " (array([0.14250266]), 2),\n",
       " (array([0.54803261]), 0),\n",
       " (array([0.5437788]), 0),\n",
       " (array([0.45196739]), 0),\n",
       " (array([0.51931939]), 0),\n",
       " (array([0.8135413]), 1),\n",
       " (array([0.32045374]), 2),\n",
       " (array([0.57178306]), 0),\n",
       " (array([0.58064516]), 0),\n",
       " (array([0.49131514]), 0),\n",
       " (array([0.45409429]), 0),\n",
       " (array([0.52250975]), 0),\n",
       " (array([0.83410138]), 1),\n",
       " (array([0.33463311]), 2),\n",
       " (array([0.46224743]), 0),\n",
       " (array([0.60723148]), 0),\n",
       " (array([0.52570011]), 0),\n",
       " (array([0.46614676]), 0),\n",
       " (array([0.60297767]), 0),\n",
       " (array([0.77419355]), 1),\n",
       " (array([0.33534208]), 2),\n",
       " (array([0.56682028]), 0),\n",
       " (array([0.51116625]), 0),\n",
       " (array([0.64126196]), 1),\n",
       " (array([0.40765686]), 2),\n",
       " (array([0.47323644]), 0),\n",
       " (array([0.91492379]), 1),\n",
       " (array([0.36192839]), 2),\n",
       " (array([0.31868132]), 2),\n",
       " (array([0.7437079]), 1),\n",
       " (array([0.56150301]), 0)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centroids for 1 days\n",
      "[[0.51880838]\n",
      " [0.74148853]\n",
      " [0.36271332]]\n",
      "centroids for 14 days\n",
      "[[0.80742129]\n",
      " [0.10976975]\n",
      " [0.39637216]]\n"
     ]
    }
   ],
   "source": [
    "centroids14 = kmeans14.cluster_centers_\n",
    "centroids = kmeans.cluster_centers_\n",
    "print('centroids for 1 days')\n",
    "print(centroids)\n",
    "print('centroids for 14 days')\n",
    "print(centroids14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Label'] =preds\n",
    "data_df.to_csv('./Dataset/FulDataset1day.csv', index=False)\n",
    "data_df14['Label'] =preds14\n",
    "data_df14.to_csv('./Dataset/FulDataset14day.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "covid = pd.read_csv('./Dataset/FulDataset1day.csv')\n",
    "Data = pd.read_csv('./Dataset/data1day.csv')\n",
    "\n",
    "covid['date'] = pd.to_datetime(covid['date'])\n",
    "Data['date'] = pd.to_datetime(Data['date'])\n",
    "\n",
    "column_names = ['date', 'tweet', 'hashtags', 'account']\n",
    "FullDataSet = pd.DataFrame(columns=column_names)\n",
    "dataModel = pd.DataFrame(columns=column_names)\n",
    "dataApply = pd.DataFrame(columns=column_names)\n",
    "dataValidate = pd.DataFrame(columns=column_names)\n",
    "counter = 0\n",
    "\n",
    "dataset = Data\n",
    "start_date = pd.to_datetime('01-01-2020')\n",
    "firstend_date = pd.to_datetime('06-01-2020')\n",
    "second_date = pd.to_datetime('06-02-2020')\n",
    "secondend_date = pd.to_datetime('09-01-2020')\n",
    "valstart_date = pd.to_datetime('09-02-2020')\n",
    "end_date = pd.to_datetime('10-08-2020')\n",
    "dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "for index, row in dataset.iterrows():\n",
    "    \n",
    "    label = covid['Label'].loc[covid['date'] == row['date']].values[0]\n",
    "\n",
    "    if  row['date'] >= start_date and row['date'] <= firstend_date:\n",
    "        dataModel.loc[counter] = row\n",
    "        FullDataSet.loc[counter] = row\n",
    "        if label == 1:\n",
    "            dataModel.loc[counter, 'label'] = 'increase'\n",
    "            FullDataSet.loc[counter, 'label'] = 'increase'\n",
    "        elif label == 0:\n",
    "            dataModel.loc[counter, 'label'] = 'equal'\n",
    "            FullDataSet.loc[counter, 'label'] = 'equal'\n",
    "        else:\n",
    "            dataModel.loc[counter, 'label'] = 'decrease'\n",
    "            FullDataSet.loc[counter, 'label'] = 'decrease'\n",
    "    elif row['date'] >= second_date and  row['date'] <= secondend_date:\n",
    "        dataApply.loc[counter] = row\n",
    "        FullDataSet.loc[counter] = row\n",
    "        if label == 1:\n",
    "            dataApply.loc[counter, 'label'] = 'increase'\n",
    "            FullDataSet.loc[counter, 'label'] = 'increase'\n",
    "        elif label == 0:\n",
    "            dataApply.loc[counter, 'label'] = 'equal'\n",
    "            FullDataSet.loc[counter, 'label'] = 'equal'\n",
    "        else:\n",
    "            dataApply.loc[counter, 'label'] = 'decrease'\n",
    "            FullDataSet.loc[counter, 'label'] = 'decrease'\n",
    "    else:\n",
    "        dataValidate.loc[counter] = row\n",
    "        FullDataSet.loc[counter] = row\n",
    "        if label == 1:\n",
    "            dataValidate.loc[counter, 'label'] = 'increase'\n",
    "            FullDataSet.loc[counter, 'label'] = 'increase'\n",
    "        elif label == 0:\n",
    "            dataValidate.loc[counter, 'label'] = 'equal'\n",
    "            FullDataSet.loc[counter, 'label'] = 'equal'\n",
    "        else:\n",
    "            dataValidate.loc[counter, 'label'] = 'decrease'\n",
    "            FullDataSet.loc[counter, 'label'] = 'decrease'\n",
    "\n",
    "    counter = counter + 1\n",
    "FullDataSet.to_csv('./ModelDataset/FullModelDay1.csv', index=False)\n",
    "dataModel.to_csv('./ModelDataset/data1dayModel.csv', index=False)\n",
    "dataApply.to_csv('./ModelDataset/data1dayApply.csv', index=False)\n",
    "dataValidate.to_csv('./ModelDataset/data1dayValidate.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "covid = pd.read_csv('./Dataset/FulDataset14day.csv')\n",
    "Data = pd.read_csv('./Dataset/data14day.csv')\n",
    "\n",
    "covid['date'] = pd.to_datetime(covid['date'])\n",
    "Data['date'] = pd.to_datetime(Data['date'])\n",
    "\n",
    "column_names = ['date', 'tweet', 'hashtags', 'account']\n",
    "FullDataSet14 = pd.DataFrame(columns=column_names)\n",
    "dataModel14 = pd.DataFrame(columns=column_names)\n",
    "dataApply14 = pd.DataFrame(columns=column_names)\n",
    "dataValidate14 = pd.DataFrame(columns=column_names)\n",
    "counter = 0\n",
    "\n",
    "dataset = Data\n",
    "start_date = pd.to_datetime('01-15-2020')\n",
    "firstend_date = pd.to_datetime('06-05-2020')\n",
    "second_date = pd.to_datetime('06-06-2020')\n",
    "secondend_date = pd.to_datetime('09-01-2020')\n",
    "dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "for index, row in dataset.iterrows():\n",
    "    \n",
    "    label = covid['Label'].loc[covid['date'] == row['date']].values[0]\n",
    "    FullDataSet14.loc[counter] = row\n",
    "    if label == 1:\n",
    "        FullDataSet14.loc[counter, 'label'] = 'increase'\n",
    "    elif label == 0:\n",
    "        FullDataSet14.loc[counter, 'label'] = 'equal'\n",
    "    else:\n",
    "        FullDataSet14.loc[counter, 'label'] = 'decrease'\n",
    "\n",
    "    if  row['date'] >= start_date and row['date'] <= firstend_date:\n",
    "        dataModel14.loc[counter] = row\n",
    "       \n",
    "        if label == 1:\n",
    "            dataModel14.loc[counter, 'label'] = 'increase'\n",
    "        elif label == 0:\n",
    "            dataModel14.loc[counter, 'label'] = 'equal'\n",
    "        else:\n",
    "            dataModel14.loc[counter, 'label'] = 'decrease'\n",
    "    elif row['date'] >= second_date and  row['date'] <= secondend_date:\n",
    "        dataApply14.loc[counter] = row\n",
    "        if label == 1:\n",
    "            dataApply14.loc[counter, 'label'] = 'increase'\n",
    "        elif label == 0:\n",
    "            dataApply14.loc[counter, 'label'] = 'equal'\n",
    "        else:\n",
    "            dataApply14.loc[counter, 'label'] = 'decrease'\n",
    "    else:\n",
    "        dataValidate14.loc[counter] = row\n",
    "        if label == 1:\n",
    "            dataValidate14.loc[counter, 'label'] = 'increase'\n",
    "        elif label == 0:\n",
    "            dataValidate14.loc[counter, 'label'] = 'equal'\n",
    "        else:\n",
    "            dataValidate14.loc[counter, 'label'] = 'decrease'\n",
    "\n",
    "    counter = counter + 1\n",
    "FullDataSet14.to_csv('./ModelDataset/FullModel14.csv', index=False)\n",
    "dataModel14.to_csv('./ModelDataset/data14dayModel.csv', index=False)\n",
    "dataApply14.to_csv('./ModelDataset/data14dayApply.csv', index=False)\n",
    "dataValidate14.to_csv('./ModelDataset/dataday14Validate.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\reza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\reza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file = './ModelDataset/FullModelDay1.csv'\n",
    "filemodel = './ModelDataset/data1dayModel.csv'\n",
    "filetest= './ModelDataset/data1dayApply.csv'\n",
    "filevalidate= './ModelDataset/data1dayValidate.csv'\n",
    "data_df = pd.read_csv(file, header=0)\n",
    "data_model = pd.read_csv(filemodel, header=0)\n",
    "data_test = pd.read_csv(filetest, header=0)\n",
    "data_validate = pd.read_csv(filevalidate, header=0)\n",
    "\n",
    "file14 = './ModelDataset/FullModel14.csv'\n",
    "filemodel14 = './ModelDataset/data14dayModel.csv'\n",
    "filetest14= './ModelDataset/data14dayApply.csv'\n",
    "filevalidate14= './ModelDataset/dataday14Validate.csv'\n",
    "data_df14 = pd.read_csv(file14, header=0)\n",
    "data_model14 = pd.read_csv(filemodel14, header=0)\n",
    "data_test14 = pd.read_csv(filetest14, header=0)\n",
    "data_validate14 = pd.read_csv(filevalidate14, header=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'(?u)\\b\\w+\\b', ngram_range=(1, 1),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_df[['tweet','label']] \n",
    "df_train = data_model[['tweet','label']] \n",
    "df_test= data_test[['tweet','label']] \n",
    "df_validate= data_validate[['tweet','label']] \n",
    "\n",
    "\n",
    "df14 = data_df14[['tweet','label']] \n",
    "df_train14 = data_model14[['tweet','label']] \n",
    "df_test14= data_test14[['tweet','label']] \n",
    "df_validate14= data_validate14[['tweet','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(Data):\n",
    "    Data = Data.rstrip('\\n')\n",
    "    Data = Data.rstrip('\\t')\n",
    "    Data = re.sub('<[^<]+?>', '', Data)\n",
    "    Data = re.sub(r'\\d+', '', Data)\n",
    "    Data = re.sub('[^0-9a-zA-Z\\w.]', ' ', Data)\n",
    "    Data = re.sub('\\s+', ' ', Data, flags=re.I)\n",
    "    Data = re.sub('\\.\\.+', ' ', Data)\n",
    "    DataReturn = \"\"\n",
    "    token = tokenize.sent_tokenize(Data)\n",
    "    for j in range(len(token)):\n",
    "        s = tokenize.word_tokenize(token[j])\n",
    "        clean_word = [\n",
    "            w.lower() for w in s\n",
    "            if w.lower() not in stop_words and w.isalnum() and len(w)>2\n",
    "        ]\n",
    "        DataReturn = DataReturn + ' '.join(clean_word) + ' . '\n",
    "    return DataReturn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 11384 articles are cleaned.\n",
      "All 7038 articles are cleaned.\n",
      "All 3265 articles are cleaned.\n",
      "All 1081 articles are cleaned.\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "n = df.shape[0]\n",
    "data_cleaned = df.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_cleaned.loc[:, 'tweet'] = pd.Series(articles, index=data_df.index)\n",
    "\n",
    "articles = []\n",
    "n = df_train.shape[0]\n",
    "data_train_cleaned = df_train.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df_train.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_train_cleaned.loc[:, 'tweet'] = pd.Series(articles, index=data_model.index)\n",
    "\n",
    "articles = []\n",
    "n = df_test.shape[0]\n",
    "data_test_cleaned = df_test.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df_test.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_test_cleaned.loc[:, 'tweet'] = pd.Series(articles, index=data_test.index)\n",
    "\n",
    "articles = []\n",
    "n = df_validate.shape[0]\n",
    "data_validate_cleaned = df_validate.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df_validate.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_validate_cleaned.loc[:, 'tweet'] = pd.Series(articles, index=data_validate.index)\n",
    "\n",
    "\n",
    "# Un-comment the following codes when the label is available\n",
    "#data_cleaned.loc[:, 'Category'] = pd.Categorical(data_cleaned.label)\n",
    "#data_cleaned['code'] = data_cleaned.Category.cat.codes\n",
    "\n",
    "#category_to_code = dict(enumerate(data_cleaned['Category'].cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 11017 articles are cleaned.\n",
      "All 7213 articles are cleaned.\n",
      "All 3085 articles are cleaned.\n",
      "All 719 articles are cleaned.\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "n = df14.shape[0]\n",
    "data_cleaned14 = df14.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df14.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_cleaned14.loc[:, 'tweet'] = pd.Series(articles, index=data_df14.index)\n",
    "\n",
    "articles = []\n",
    "n = df_train14.shape[0]\n",
    "data_train_cleaned14 = df_train14.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df_train14.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_train_cleaned14.loc[:, 'tweet'] = pd.Series(articles, index=data_model14.index)\n",
    "\n",
    "articles = []\n",
    "n = df_test14.shape[0]\n",
    "data_test_cleaned14 = df_test14.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df_test14.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_test_cleaned14.loc[:, 'tweet'] = pd.Series(articles, index=data_test14.index)\n",
    "\n",
    "articles = []\n",
    "n = df_validate14.shape[0]\n",
    "data_validate_cleaned14 = df_validate14.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df_validate14.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_validate_cleaned14.loc[:, 'tweet'] = pd.Series(articles, index=data_validate14.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dementia community investment looking evidence...</td>\n",
       "      <td>equal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>latest research health promotion chronic disea...</td>\n",
       "      <td>equal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dedicated protecting canadians covid . must tr...</td>\n",
       "      <td>equal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>become fluwatcher help track flu covid . fast ...</td>\n",
       "      <td>equal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avoid red stay safe protect family community c...</td>\n",
       "      <td>equal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>kids likely take care something ownership way ...</td>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>biden take aim trump handling coronavirus scho...</td>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>union says also ask quebec superior court forc...</td>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>steroids cheap readily available medication an...</td>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>coronavirus australia enters recession posts w...</td>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>719 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet     label\n",
       "0    dementia community investment looking evidence...     equal\n",
       "1    latest research health promotion chronic disea...     equal\n",
       "2    dedicated protecting canadians covid . must tr...     equal\n",
       "3    become fluwatcher help track flu covid . fast ...     equal\n",
       "4    avoid red stay safe protect family community c...     equal\n",
       "..                                                 ...       ...\n",
       "714  kids likely take care something ownership way ...  decrease\n",
       "715  biden take aim trump handling coronavirus scho...  decrease\n",
       "716  union says also ask quebec superior court forc...  decrease\n",
       "717  steroids cheap readily available medication an...  decrease\n",
       "718  coronavirus australia enters recession posts w...  decrease\n",
       "\n",
       "[719 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_validate_cleaned14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.fit(data_cleaned['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors= vectors.transform(data_train_cleaned['tweet'])\n",
    "test_vectors= vectors.transform(data_test_cleaned['tweet'])\n",
    "validate_vectors= vectors.transform(data_validate_cleaned['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors14= vectors.transform(data_train_cleaned14['tweet'])\n",
    "test_vectors14= vectors.transform(data_test_cleaned14['tweet'])\n",
    "validate_vectors14= vectors.transform(data_validate_cleaned14['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Linear SVM\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\"]\n",
    "classifiers = [\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    ]\n",
    "\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    \n",
    "    clf.fit(train_vectors.toarray(), data_train_cleaned['label'])\n",
    "    score = clf.score(validate_vectors.toarray(), data_validate_cleaned['label'])\n",
    "    print(f'Score of {name} for validation dataset is : {score}')\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(test_vectors.toarray())\n",
    "    else:\n",
    "        Z = clf.predict_proba(test_vectors.toarray())\n",
    "    print(f'The predicted probability of {name} for test data is: {Z}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
