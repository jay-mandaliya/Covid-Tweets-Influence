{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-b3d1071a5684>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  truncated_covid['new_recovered'] = truncated_covid['new_recovered'].fillna(0)\n",
      "<ipython-input-3-b3d1071a5684>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  truncated_covid['cumulative_recovered'] = truncated_covid['cumulative_recovered'].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_covid = pd.read_csv('./data/covid.csv', dtype={'datacommons_id': str})\n",
    "\n",
    "selected_fields = ['date', 'new_confirmed', 'new_recovered', 'cumulative_confirmed', 'cumulative_recovered']\n",
    "selected_covid = raw_covid[selected_fields]\n",
    "\n",
    "truncated_covid = selected_covid.head(283);\n",
    "\n",
    "truncated_covid['new_recovered'] = truncated_covid['new_recovered'].fillna(0)\n",
    "truncated_covid['cumulative_recovered'] = truncated_covid['cumulative_recovered'].fillna(0)\n",
    "\n",
    "truncated_covid.to_csv('./Stage2/covid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-fb08c914852e>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_govcanhealth['date'] = pd.to_datetime(selected_govcanhealth['date'])\n",
      "<ipython-input-5-fb08c914852e>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_cdcgov['date'] = pd.to_datetime(selected_cdcgov['date'])\n",
      "<ipython-input-5-fb08c914852e>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_cbcnews['date'] = pd.to_datetime(selected_cbcnews['date'])\n",
      "<ipython-input-5-fb08c914852e>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_globalnews['date'] = pd.to_datetime(selected_globalnews['date'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_govcanhealth = pd.read_csv('./data/govcanhealth.csv')\n",
    "raw_cdcgov = pd.read_csv('./data/cdcgov.csv')\n",
    "raw_cbcnews = pd.read_csv('./data/cbcnews.csv')\n",
    "raw_globalnews = pd.read_csv('./data/globalnews.csv')\n",
    "\n",
    "selected_fields = ['date', 'tweet', 'hashtags',]\n",
    "selected_govcanhealth = raw_govcanhealth[selected_fields]\n",
    "selected_cdcgov = raw_cdcgov[selected_fields]\n",
    "selected_cbcnews = raw_cbcnews[selected_fields]\n",
    "selected_globalnews = raw_globalnews[selected_fields]\n",
    "\n",
    "selected_govcanhealth['date'] = pd.to_datetime(selected_govcanhealth['date'])\n",
    "selected_cdcgov['date'] = pd.to_datetime(selected_cdcgov['date'])\n",
    "selected_cbcnews['date'] = pd.to_datetime(selected_cbcnews['date'])\n",
    "selected_globalnews['date'] = pd.to_datetime(selected_globalnews['date'])\n",
    "\n",
    "start_date = '11-01-2019'\n",
    "end_date = '10-09-2020'\n",
    "mask_govcanhealth = (selected_govcanhealth['date'] >= start_date) & (selected_govcanhealth['date'] <= end_date)\n",
    "mask_cdcgov = (selected_cdcgov['date'] >= start_date) & (selected_cdcgov['date'] <= end_date)\n",
    "mask_cbcnews = (selected_cbcnews['date'] >= start_date) & (selected_cbcnews['date'] <= end_date)\n",
    "mask_globalnews = (selected_globalnews['date'] >= start_date) & (selected_globalnews['date'] <= end_date)\n",
    "\n",
    "truncated_govcanhealth = selected_govcanhealth.loc[mask_govcanhealth]\n",
    "truncated_cdcgov = selected_cdcgov.loc[mask_cdcgov]\n",
    "truncated_cbcnews = selected_cbcnews.loc[mask_cbcnews]\n",
    "truncated_globalnews = selected_globalnews.loc[mask_globalnews]\n",
    "\n",
    "truncated_govcanhealth.to_csv('./Stage2/govcanhealth.csv', index=False)\n",
    "truncated_cdcgov.to_csv('./Stage2/cdcgov.csv', index=False)\n",
    "truncated_cbcnews.to_csv('./Stage2/cbcnews.csv', index=False)\n",
    "truncated_globalnews.to_csv('./Stage2/globalnews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_govcanhealth = pd.read_csv('./Stage2/govcanhealth.csv')\n",
    "raw_cdcgov = pd.read_csv('./Stage2/cdcgov.csv')\n",
    "raw_cbcnews = pd.read_csv('./Stage2/cbcnews.csv')\n",
    "raw_globalnews = pd.read_csv('./Stage2/globalnews.csv')\n",
    "\n",
    "keywords = ['covid', 'physicaldistancing', 'publichealth', 'coronavirus', 'pandemic', 'mask']\n",
    "pattern = '|'.join(keywords)\n",
    "\n",
    "filtered_govcanhealth = raw_govcanhealth[raw_govcanhealth['tweet'].str.contains(pattern, case=False)]\n",
    "filtered_cdcgov = raw_cdcgov[raw_cdcgov['tweet'].str.contains(pattern, case=False)]\n",
    "filtered_cbcnews = raw_cbcnews[raw_cbcnews['tweet'].str.contains(pattern, case=False)]\n",
    "filtered_globalnews = raw_globalnews[raw_globalnews['tweet'].str.contains(pattern, case=False)]\n",
    "\n",
    "filtered_govcanhealth.to_csv('./Stage3/govcanhealth.csv', index=False)\n",
    "filtered_cdcgov.to_csv('./Stage3/cdcgov.csv', index=False)\n",
    "filtered_cbcnews.to_csv('./Stage3/cbcnews.csv', index=False)\n",
    "filtered_globalnews.to_csv('./Stage3/globalnews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date  new_confirmed  new_recovered  cumulative_confirmed  \\\n",
      "0   2020-01-01            0.0            0.0                   0.0   \n",
      "1   2020-01-02            0.0            0.0                   0.0   \n",
      "2   2020-01-03            0.0            0.0                   0.0   \n",
      "3   2020-01-04            0.0            0.0                   0.0   \n",
      "4   2020-01-05            0.0            0.0                   0.0   \n",
      "..         ...            ...            ...                   ...   \n",
      "277 2020-10-04         1685.0         1376.0              166156.0   \n",
      "278 2020-10-05         2804.0         2091.0              168960.0   \n",
      "279 2020-10-06         2363.0         1660.0              171323.0   \n",
      "280 2020-10-07         1800.0         1672.0              173123.0   \n",
      "281 2020-10-08         2436.0         1842.0              175559.0   \n",
      "\n",
      "     cumulative_recovered  \n",
      "0                     0.0  \n",
      "1                     0.0  \n",
      "2                     0.0  \n",
      "3                     0.0  \n",
      "4                     0.0  \n",
      "..                    ...  \n",
      "277              140243.0  \n",
      "278              142334.0  \n",
      "279              143994.0  \n",
      "280              145666.0  \n",
      "281              147508.0  \n",
      "\n",
      "[282 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "covid = pd.read_csv('./Stage2/covid.csv')\n",
    "govcanhealth = pd.read_csv('./Stage3/govcanhealth.csv')\n",
    "cdcgov = pd.read_csv('./Stage3/cdcgov.csv')\n",
    "cbcnews = pd.read_csv('./Stage3/cbcnews.csv')\n",
    "globalnews = pd.read_csv('./Stage3/globalnews.csv')\n",
    "\n",
    "covid['date'] = pd.to_datetime(covid['date'])\n",
    "govcanhealth['date'] = pd.to_datetime(govcanhealth['date'])\n",
    "cdcgov['date'] = pd.to_datetime(cdcgov['date'])\n",
    "cbcnews['date'] = pd.to_datetime(cbcnews['date'])\n",
    "globalnews['date'] = pd.to_datetime(globalnews['date'])\n",
    "\n",
    "start_date = '01-01-2020'\n",
    "end_date = '10-08-2020'\n",
    "mask_govcanhealth = (govcanhealth['date'] >= start_date) & (govcanhealth['date'] <= end_date)\n",
    "mask_cdcgov = (cdcgov['date'] >= start_date) & (cdcgov['date'] <= end_date)\n",
    "mask_cbcnews = (cbcnews['date'] >= start_date) & (cbcnews['date'] <= end_date)\n",
    "mask_globalnews = (globalnews['date'] >= start_date) & (globalnews['date'] <= end_date)\n",
    "mask_covid = (covid['date'] >= start_date) & (covid['date'] <= end_date)\n",
    "govcanhealth = govcanhealth.loc[mask_govcanhealth]\n",
    "cdcgov = cdcgov.loc[mask_cdcgov]\n",
    "cbcnews = cbcnews.loc[mask_cbcnews]\n",
    "globalnews = globalnews.loc[mask_globalnews]\n",
    "covid1 = covid.loc[mask_covid]\n",
    "column_names = ['date', 'tweet', 'hashtags', 'Changes', 'account']\n",
    "column_Covidnames = ['date','Changes']\n",
    "data = pd.DataFrame(columns=column_names)\n",
    "coviddata = pd.DataFrame(columns=column_Covidnames)\n",
    "counter = 0\n",
    "CovidDay= 0\n",
    "\n",
    "print(covid1)\n",
    "\n",
    "for index, row in covid1.iterrows():\n",
    "    next_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date'] + timedelta(days=1)].values[0]\n",
    "    same_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date']].values[0]\n",
    "\n",
    "    coviddata.loc[CovidDay] = row\n",
    "    coviddata.loc[CovidDay, 'Changes'] = next_day_cases - same_day_cases\n",
    "\n",
    "    CovidDay = CovidDay + 1\n",
    "\n",
    "for index, row in govcanhealth.iterrows():\n",
    "    next_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date'] + timedelta(days=1)].values[0]\n",
    "    same_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date']].values[0]\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'govcanhealth'\n",
    "    data.loc[counter, 'Changes'] = next_day_cases - same_day_cases\n",
    "    counter = counter + 1\n",
    "\n",
    "\n",
    "for index, row in cdcgov.iterrows():\n",
    "    next_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date'] + timedelta(days=1)].values[0]\n",
    "    same_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date']].values[0]\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'cdcgov'\n",
    "    data.loc[counter, 'Changes'] = next_day_cases - same_day_cases\n",
    "\n",
    "    counter = counter + 1\n",
    "\n",
    "for index, row in cbcnews.iterrows():\n",
    "    next_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date'] + timedelta(days=1)].values[0]\n",
    "    same_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date']].values[0]\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'cbcnews'\n",
    "\n",
    "    data.loc[counter, 'Changes'] = next_day_cases - same_day_cases\n",
    "\n",
    "    counter = counter + 1\n",
    "\n",
    "for index, row in globalnews.iterrows():\n",
    "    next_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date'] + timedelta(days=1)].values[0]\n",
    "    same_day_cases = covid['new_confirmed'].loc[covid['date'] == row['date']].values[0]\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'globalnews'\n",
    "\n",
    "    data.loc[counter, 'Changes'] = next_day_cases - same_day_cases\n",
    "\n",
    "    counter = counter + 1\n",
    "\n",
    "data.to_csv('./Dataset/data1day.csv', index=False)\n",
    "coviddata.to_csv('./Dataset/covid1day.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "covid = pd.read_csv('./Stage2/covid.csv')\n",
    "govcanhealth = pd.read_csv('./Stage3/govcanhealth.csv')\n",
    "cdcgov = pd.read_csv('./Stage3/cdcgov.csv')\n",
    "cbcnews = pd.read_csv('./Stage3/cbcnews.csv')\n",
    "globalnews = pd.read_csv('./Stage3/globalnews.csv')\n",
    "\n",
    "\n",
    "covid['date'] = pd.to_datetime(covid['date'])\n",
    "govcanhealth['date'] = pd.to_datetime(govcanhealth['date'])\n",
    "cdcgov['date'] = pd.to_datetime(cdcgov['date'])\n",
    "cbcnews['date'] = pd.to_datetime(cbcnews['date'])\n",
    "globalnews['date'] = pd.to_datetime(globalnews['date'])\n",
    "\n",
    "start_date = '01-15-2020'\n",
    "end_date = '09-25-2020'\n",
    "mask_govcanhealth = (govcanhealth['date'] >= start_date) & (govcanhealth['date'] <= end_date)\n",
    "mask_cdcgov = (cdcgov['date'] >= start_date) & (cdcgov['date'] <= end_date)\n",
    "mask_cbcnews = (cbcnews['date'] >= start_date) & (cbcnews['date'] <= end_date)\n",
    "mask_globalnews = (globalnews['date'] >= start_date) & (globalnews['date'] <= end_date)\n",
    "covid_mask = (covid['date'] >= start_date) & (covid['date'] <= end_date)\n",
    "govcanhealth = govcanhealth.loc[mask_govcanhealth]\n",
    "cdcgov = cdcgov.loc[mask_cdcgov]\n",
    "cbcnews = cbcnews.loc[mask_cbcnews]\n",
    "globalnews = globalnews.loc[mask_globalnews]\n",
    "covid1 = covid.loc[covid_mask]\n",
    "column_names = ['date', 'tweet', 'hashtags', 'Changes', 'account']\n",
    "column_Covidnames = ['date','Changes']\n",
    "data = pd.DataFrame(columns=column_names)\n",
    "coviddata = pd.DataFrame(columns=column_Covidnames)\n",
    "counter = 0\n",
    "CovidDay= 0\n",
    "\n",
    "\n",
    "\n",
    "def next_14_cases(date):\n",
    "    addition = 0\n",
    "    for i in range(14):\n",
    "        addition = addition + covid['new_confirmed'].loc[covid['date'] == date + timedelta(days=i+1)].values[0]\n",
    "\n",
    "    return addition\n",
    "\n",
    "\n",
    "def previous_14_cases(date):\n",
    "    addition = 0\n",
    "    for i in range(14):\n",
    "        addition = addition + covid['new_confirmed'].loc[covid['date'] == date - timedelta(days=i)].values[0]\n",
    "\n",
    "    return addition\n",
    "\n",
    "\n",
    "for index, row in covid1.iterrows():\n",
    "    next_cases = next_14_cases(row['date'])\n",
    "    previous_cases = previous_14_cases(row['date'])\n",
    "    coviddata.loc[CovidDay] = row\n",
    "    coviddata.loc[CovidDay, 'Changes'] = next_cases - previous_cases\n",
    "\n",
    "    CovidDay = CovidDay + 1\n",
    "\n",
    "for index, row in govcanhealth.iterrows():\n",
    "    next_cases = next_14_cases(row['date'])\n",
    "    previous_cases = previous_14_cases(row['date'])\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'govcanhealth'\n",
    "\n",
    "    data.loc[counter, 'Changes'] = next_cases - previous_cases\n",
    "\n",
    "    counter = counter + 1\n",
    "\n",
    "\n",
    "for index, row in cdcgov.iterrows():\n",
    "    next_cases = next_14_cases(row['date'])\n",
    "    previous_cases = previous_14_cases(row['date'])\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'cdcgov'\n",
    "\n",
    "    data.loc[counter, 'Changes'] = next_cases - previous_cases\n",
    "\n",
    "    counter = counter + 1\n",
    "\n",
    "for index, row in cbcnews.iterrows():\n",
    "    next_cases = next_14_cases(row['date'])\n",
    "    previous_cases = previous_14_cases(row['date'])\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'cbcnews'\n",
    "\n",
    "    data.loc[counter, 'Changes'] = next_cases - previous_cases\n",
    "\n",
    "    counter = counter + 1\n",
    "\n",
    "for index, row in globalnews.iterrows():\n",
    "    next_cases = next_14_cases(row['date'])\n",
    "    previous_cases = previous_14_cases(row['date'])\n",
    "\n",
    "    data.loc[counter] = row\n",
    "    data.loc[counter, 'account'] = 'globalnews'\n",
    "\n",
    "    data.loc[counter, 'Changes'] = next_cases - previous_cases\n",
    "\n",
    "    counter = counter + 1\n",
    "\n",
    "data.to_csv('./Dataset/data14day.csv', index=False)\n",
    "\n",
    "coviddata.to_csv('./Dataset/Covid14day.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans14 = KMeans(n_clusters=3, random_state=0)\n",
    "file = 'Dataset/covid1day.csv'\n",
    "file14 = 'Dataset/covid14day.csv'\n",
    "data_df14 = pd.read_csv(file14)\n",
    "data_df = pd.read_csv(file)\n",
    "data_Change14 = data_df14['Changes']\n",
    "data_Change = data_df['Changes']\n",
    "scaler14 = MinMaxScaler()\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_Change.to_numpy().reshape(-1,1))\n",
    "scaler14.fit(data_Change14.to_numpy().reshape(-1,1))\n",
    "normalized_Change = scaler.transform(data_Change.to_numpy().reshape(-1,1))\n",
    "normalized_Change14 = scaler14.transform(data_Change14.to_numpy().reshape(-1,1))\n",
    "data_df14['NormalChange'] =normalized_Change14\n",
    "data_df['NormalChange'] =normalized_Change\n",
    "\n",
    "data_df14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter \n",
    "preds = kmeans.fit_predict(data_df['NormalChange'].to_numpy().reshape(-1,1))\n",
    "preds14 = kmeans14.fit_predict(data_df14['NormalChange'].to_numpy().reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids14 = kmeans14.cluster_centers_\n",
    "centroids = kmeans.cluster_centers_\n",
    "print('centroids for 1 days')\n",
    "print(centroids)\n",
    "print('centroids for 14 days')\n",
    "print(centroids14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Label'] =preds\n",
    "data_df.to_csv('./Dataset/FulDataset1day.csv', index=False)\n",
    "data_df14['Label'] =preds14\n",
    "data_df14.to_csv('./Dataset/FulDataset14day.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "covid = pd.read_csv('./Dataset/FulDataset1day.csv')\n",
    "Data = pd.read_csv('./Dataset/data1day.csv')\n",
    "\n",
    "covid['date'] = pd.to_datetime(covid['date'])\n",
    "Data['date'] = pd.to_datetime(Data['date'])\n",
    "\n",
    "column_names = ['date', 'tweet', 'hashtags', 'account']\n",
    "FullDataSet = pd.DataFrame(columns=column_names)\n",
    "dataModel = pd.DataFrame(columns=column_names)\n",
    "dataApply = pd.DataFrame(columns=column_names)\n",
    "dataValidate = pd.DataFrame(columns=column_names)\n",
    "counter = 0\n",
    "\n",
    "dataset = Data\n",
    "start_date = pd.to_datetime('01-01-2020')\n",
    "firstend_date = pd.to_datetime('06-01-2020')\n",
    "second_date = pd.to_datetime('06-02-2020')\n",
    "secondend_date = pd.to_datetime('09-01-2020')\n",
    "valstart_date = pd.to_datetime('09-02-2020')\n",
    "end_date = pd.to_datetime('10-08-2020')\n",
    "dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "for index, row in dataset.iterrows():\n",
    "    \n",
    "    label = covid['Label'].loc[covid['date'] == row['date']].values[0]\n",
    "\n",
    "    if  row['date'] >= start_date and row['date'] <= firstend_date:\n",
    "        dataModel.loc[counter] = row\n",
    "        FullDataSet.loc[counter] = row\n",
    "        if label == 1:\n",
    "            dataModel.loc[counter, 'label'] = 'increase'\n",
    "            FullDataSet.loc[counter, 'label'] = 'increase'\n",
    "        elif label == 0:\n",
    "            dataModel.loc[counter, 'label'] = 'equal'\n",
    "            FullDataSet.loc[counter, 'label'] = 'equal'\n",
    "        else:\n",
    "            dataModel.loc[counter, 'label'] = 'decrease'\n",
    "            FullDataSet.loc[counter, 'label'] = 'decrease'\n",
    "    elif row['date'] >= second_date and  row['date'] <= secondend_date:\n",
    "        dataApply.loc[counter] = row\n",
    "        FullDataSet.loc[counter] = row\n",
    "        if label == 1:\n",
    "            dataApply.loc[counter, 'label'] = 'increase'\n",
    "            FullDataSet.loc[counter, 'label'] = 'increase'\n",
    "        elif label == 0:\n",
    "            dataApply.loc[counter, 'label'] = 'equal'\n",
    "            FullDataSet.loc[counter, 'label'] = 'equal'\n",
    "        else:\n",
    "            dataApply.loc[counter, 'label'] = 'decrease'\n",
    "            FullDataSet.loc[counter, 'label'] = 'decrease'\n",
    "    else:\n",
    "        dataValidate.loc[counter] = row\n",
    "        FullDataSet.loc[counter] = row\n",
    "        if label == 1:\n",
    "            dataValidate.loc[counter, 'label'] = 'increase'\n",
    "            FullDataSet.loc[counter, 'label'] = 'increase'\n",
    "        elif label == 0:\n",
    "            dataValidate.loc[counter, 'label'] = 'equal'\n",
    "            FullDataSet.loc[counter, 'label'] = 'equal'\n",
    "        else:\n",
    "            dataValidate.loc[counter, 'label'] = 'decrease'\n",
    "            FullDataSet.loc[counter, 'label'] = 'decrease'\n",
    "\n",
    "    counter = counter + 1\n",
    "FullDataSet.to_csv('./ModelDataset/FullModelDay1.csv', index=False)\n",
    "dataModel.to_csv('./ModelDataset/data1dayModel.csv', index=False)\n",
    "dataApply.to_csv('./ModelDataset/data1dayApply.csv', index=False)\n",
    "dataValidate.to_csv('./ModelDataset/data1dayValidate.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "covid = pd.read_csv('./Dataset/FulDataset14day.csv')\n",
    "Data = pd.read_csv('./Dataset/data14day.csv')\n",
    "\n",
    "covid['date'] = pd.to_datetime(covid['date'])\n",
    "Data['date'] = pd.to_datetime(Data['date'])\n",
    "\n",
    "column_names = ['date', 'tweet', 'hashtags', 'account']\n",
    "FullDataSet14 = pd.DataFrame(columns=column_names)\n",
    "dataModel14 = pd.DataFrame(columns=column_names)\n",
    "dataApply14 = pd.DataFrame(columns=column_names)\n",
    "dataValidate14 = pd.DataFrame(columns=column_names)\n",
    "counter = 0\n",
    "\n",
    "dataset = Data\n",
    "start_date = pd.to_datetime('01-15-2020')\n",
    "firstend_date = pd.to_datetime('06-05-2020')\n",
    "second_date = pd.to_datetime('06-06-2020')\n",
    "secondend_date = pd.to_datetime('09-01-2020')\n",
    "dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "for index, row in dataset.iterrows():\n",
    "    \n",
    "    label = covid['Label'].loc[covid['date'] == row['date']].values[0]\n",
    "    FullDataSet14.loc[counter] = row\n",
    "    if label == 1:\n",
    "        FullDataSet14.loc[counter, 'label'] = 'increase'\n",
    "    elif label == 0:\n",
    "        FullDataSet14.loc[counter, 'label'] = 'equal'\n",
    "    else:\n",
    "        FullDataSet14.loc[counter, 'label'] = 'decrease'\n",
    "\n",
    "    if  row['date'] >= start_date and row['date'] <= firstend_date:\n",
    "        dataModel14.loc[counter] = row\n",
    "       \n",
    "        if label == 1:\n",
    "            dataModel14.loc[counter, 'label'] = 'increase'\n",
    "        elif label == 0:\n",
    "            dataModel14.loc[counter, 'label'] = 'equal'\n",
    "        else:\n",
    "            dataModel14.loc[counter, 'label'] = 'decrease'\n",
    "    elif row['date'] >= second_date and  row['date'] <= secondend_date:\n",
    "        dataApply14.loc[counter] = row\n",
    "        if label == 1:\n",
    "            dataApply14.loc[counter, 'label'] = 'increase'\n",
    "        elif label == 0:\n",
    "            dataApply14.loc[counter, 'label'] = 'equal'\n",
    "        else:\n",
    "            dataApply14.loc[counter, 'label'] = 'decrease'\n",
    "    else:\n",
    "        dataValidate14.loc[counter] = row\n",
    "        if label == 1:\n",
    "            dataValidate14.loc[counter, 'label'] = 'increase'\n",
    "        elif label == 0:\n",
    "            dataValidate14.loc[counter, 'label'] = 'equal'\n",
    "        else:\n",
    "            dataValidate14.loc[counter, 'label'] = 'decrease'\n",
    "\n",
    "    counter = counter + 1\n",
    "FullDataSet14.to_csv('./ModelDataset/FullModel14.csv', index=False)\n",
    "dataModel14.to_csv('./ModelDataset/data14dayModel.csv', index=False)\n",
    "dataApply14.to_csv('./ModelDataset/data14dayApply.csv', index=False)\n",
    "dataValidate14.to_csv('./ModelDataset/dataday14Validate.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\reza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\reza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file = './ModelDataset/FullModelDay1.csv'\n",
    "filemodel = './ModelDataset/data1dayModel.csv'\n",
    "filetest= './ModelDataset/data1dayApply.csv'\n",
    "filevalidate= './ModelDataset/data1dayValidate.csv'\n",
    "data_df = pd.read_csv(file, header=0)\n",
    "data_model = pd.read_csv(filemodel, header=0)\n",
    "data_test = pd.read_csv(filetest, header=0)\n",
    "data_validate = pd.read_csv(filevalidate, header=0)\n",
    "\n",
    "file14 = './ModelDataset/FullModel14.csv'\n",
    "filemodel14 = './ModelDataset/data14dayModel.csv'\n",
    "filetest14= './ModelDataset/data14dayApply.csv'\n",
    "filevalidate14= './ModelDataset/dataday14Validate.csv'\n",
    "data_df14 = pd.read_csv(file14, header=0)\n",
    "data_model14 = pd.read_csv(filemodel14, header=0)\n",
    "data_test14 = pd.read_csv(filetest14, header=0)\n",
    "data_validate14 = pd.read_csv(filevalidate14, header=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'(?u)\\b\\w+\\b', ngram_range=(1, 1),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_df[['tweet','label']] \n",
    "df_train = data_model[['tweet','label']] \n",
    "df_test= data_test[['tweet','label']] \n",
    "df_validate= data_validate[['tweet','label']] \n",
    "\n",
    "\n",
    "df14 = data_df14[['tweet','label']] \n",
    "df_train14 = data_model14[['tweet','label']] \n",
    "df_test14= data_test14[['tweet','label']] \n",
    "df_validate14= data_validate14[['tweet','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(Data):\n",
    "    Data = Data.rstrip('\\n')\n",
    "    Data = Data.rstrip('\\t')\n",
    "    Data = re.sub('<[^<]+?>', '', Data)\n",
    "    Data = re.sub(r'\\d+', '', Data)\n",
    "    Data = re.sub('[^0-9a-zA-Z\\w.]', ' ', Data)\n",
    "    Data = re.sub('\\s+', ' ', Data, flags=re.I)\n",
    "    Data = re.sub('\\.\\.+', ' ', Data)\n",
    "    DataReturn = \"\"\n",
    "    token = tokenize.sent_tokenize(Data)\n",
    "    for j in range(len(token)):\n",
    "        s = tokenize.word_tokenize(token[j])\n",
    "        clean_word = [\n",
    "            w.lower() for w in s\n",
    "            if w.lower() not in stop_words and w.isalnum() and len(w)>2\n",
    "        ]\n",
    "        DataReturn = DataReturn + ' '.join(clean_word) + ' . '\n",
    "    return DataReturn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 11384 articles are cleaned.\n",
      "All 7038 articles are cleaned.\n",
      "All 3265 articles are cleaned.\n",
      "All 1081 articles are cleaned.\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "n = df.shape[0]\n",
    "data_cleaned = df.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_cleaned.loc[:, 'tweet'] = pd.Series(articles, index=data_df.index)\n",
    "\n",
    "articles = []\n",
    "n = df_train.shape[0]\n",
    "data_train_cleaned = df_train.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df_train.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_train_cleaned.loc[:, 'tweet'] = pd.Series(articles, index=data_model.index)\n",
    "\n",
    "articles = []\n",
    "n = df_test.shape[0]\n",
    "data_test_cleaned = df_test.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df_test.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_test_cleaned.loc[:, 'tweet'] = pd.Series(articles, index=data_test.index)\n",
    "\n",
    "articles = []\n",
    "n = df_validate.shape[0]\n",
    "data_validate_cleaned = df_validate.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df_validate.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_validate_cleaned.loc[:, 'tweet'] = pd.Series(articles, index=data_validate.index)\n",
    "\n",
    "\n",
    "# Un-comment the following codes when the label is available\n",
    "#data_cleaned.loc[:, 'Category'] = pd.Categorical(data_cleaned.label)\n",
    "#data_cleaned['code'] = data_cleaned.Category.cat.codes\n",
    "\n",
    "#category_to_code = dict(enumerate(data_cleaned['Category'].cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 11017 articles are cleaned.\n",
      "All 7213 articles are cleaned.\n",
      "All 3085 articles are cleaned.\n",
      "All 719 articles are cleaned.\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "n = df14.shape[0]\n",
    "data_cleaned14 = df14.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df14.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_cleaned14.loc[:, 'tweet'] = pd.Series(articles, index=data_df14.index)\n",
    "\n",
    "articles = []\n",
    "n = df_train14.shape[0]\n",
    "data_train_cleaned14 = df_train14.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df_train14.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_train_cleaned14.loc[:, 'tweet'] = pd.Series(articles, index=data_model14.index)\n",
    "\n",
    "articles = []\n",
    "n = df_test14.shape[0]\n",
    "data_test_cleaned14 = df_test14.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df_test14.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_test_cleaned14.loc[:, 'tweet'] = pd.Series(articles, index=data_test14.index)\n",
    "\n",
    "articles = []\n",
    "n = df_validate14.shape[0]\n",
    "data_validate_cleaned14 = df_validate14.copy()\n",
    "for i in range(n):\n",
    "    temp = cleaning(df_validate14.iloc[i]['tweet'])\n",
    "    articles.append(temp)\n",
    "print(\"All {} articles are cleaned.\".format(n))\n",
    "data_validate_cleaned14.loc[:, 'tweet'] = pd.Series(articles, index=data_validate14.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dementia community investment looking evidence...</td>\n",
       "      <td>equal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>latest research health promotion chronic disea...</td>\n",
       "      <td>equal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dedicated protecting canadians covid . must tr...</td>\n",
       "      <td>equal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>become fluwatcher help track flu covid . fast ...</td>\n",
       "      <td>equal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avoid red stay safe protect family community c...</td>\n",
       "      <td>equal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>kids likely take care something ownership way ...</td>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>biden take aim trump handling coronavirus scho...</td>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>union says also ask quebec superior court forc...</td>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>steroids cheap readily available medication an...</td>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>coronavirus australia enters recession posts w...</td>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>719 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet     label\n",
       "0    dementia community investment looking evidence...     equal\n",
       "1    latest research health promotion chronic disea...     equal\n",
       "2    dedicated protecting canadians covid . must tr...     equal\n",
       "3    become fluwatcher help track flu covid . fast ...     equal\n",
       "4    avoid red stay safe protect family community c...     equal\n",
       "..                                                 ...       ...\n",
       "714  kids likely take care something ownership way ...  decrease\n",
       "715  biden take aim trump handling coronavirus scho...  decrease\n",
       "716  union says also ask quebec superior court forc...  decrease\n",
       "717  steroids cheap readily available medication an...  decrease\n",
       "718  coronavirus australia enters recession posts w...  decrease\n",
       "\n",
       "[719 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_validate_cleaned14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.fit(data_cleaned['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors= vectors.transform(data_train_cleaned['tweet'])\n",
    "test_vectors= vectors.transform(data_test_cleaned['tweet'])\n",
    "validate_vectors= vectors.transform(data_validate_cleaned['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors14= vectors.transform(data_train_cleaned14['tweet'])\n",
    "test_vectors14= vectors.transform(data_test_cleaned14['tweet'])\n",
    "validate_vectors14= vectors.transform(data_validate_cleaned14['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7038x25890 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 105481 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM\n",
      "Score of Linear SVM for validation dataset is : 0.6114708603145236\n",
      "The predicted probability of Linear SVM for test data is: [[ 0.99522586  2.22224452 -0.22170395]\n",
      " [ 0.99102388  2.22228971 -0.22125649]\n",
      " [ 0.98330431  2.22199576 -0.21999974]\n",
      " ...\n",
      " [ 0.9949879   2.22217642 -0.22160766]\n",
      " [ 0.98955909  2.2223078  -0.22109901]\n",
      " [ 0.99304852  2.22208292 -0.22128644]]\n",
      "Decision Tree\n",
      "Score of Decision Tree for validation dataset is : 0.6086956521739131\n",
      "The predicted probability of Decision Tree for test data is: [[0.06977409 0.8781813  0.05204461]\n",
      " [0.06977409 0.8781813  0.05204461]\n",
      " [0.06977409 0.8781813  0.05204461]\n",
      " ...\n",
      " [0.06977409 0.8781813  0.05204461]\n",
      " [0.06977409 0.8781813  0.05204461]\n",
      " [0.06977409 0.8781813  0.05204461]]\n",
      "Random Forest\n",
      "Score of Random Forest for validation dataset is : 0.6114708603145236\n",
      "The predicted probability of Random Forest for test data is: [[0.07072344 0.87657582 0.05270074]\n",
      " [0.07072344 0.87657582 0.05270074]\n",
      " [0.06358059 0.88903661 0.0473828 ]\n",
      " ...\n",
      " [0.07072344 0.87657582 0.05270074]\n",
      " [0.07072344 0.87657582 0.05270074]\n",
      " [0.07072344 0.87657582 0.05270074]]\n",
      "Neural Net\n",
      "Score of Neural Net for validation dataset is : 0.6114708603145236\n",
      "The predicted probability of Neural Net for test data is: [[0.06498065 0.88643312 0.04858623]\n",
      " [0.07023703 0.87632967 0.0534333 ]\n",
      " [0.07162119 0.87364011 0.0547387 ]\n",
      " ...\n",
      " [0.06717902 0.88222451 0.05059648]\n",
      " [0.06624976 0.8839821  0.04976814]\n",
      " [0.06979945 0.8771732  0.05302734]]\n",
      "AdaBoost\n",
      "Score of AdaBoost for validation dataset is : 0.6022201665124884\n",
      "The predicted probability of AdaBoost for test data is: [[-0.01852228  0.0709013  -0.05237902]\n",
      " [-0.01594866  0.07747767 -0.06152901]\n",
      " [-1.89410822  0.97171527  0.92239295]\n",
      " ...\n",
      " [-0.01594866  0.07747767 -0.06152901]\n",
      " [-0.0277029   0.08225533 -0.05455243]\n",
      " [-0.0277029   0.08225533 -0.05455243]]\n",
      "Naive Bayes\n",
      "Score of Naive Bayes for validation dataset is : 0.5494912118408881\n",
      "The predicted probability of Naive Bayes for test data is: [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Linear SVM\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\"]\n",
    "classifiers = [\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    ]\n",
    "\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print(name)\n",
    "    clf.fit(train_vectors.toarray(), data_train_cleaned['label'])\n",
    "    score = clf.score(validate_vectors.toarray(), data_validate_cleaned['label'])\n",
    "    print(f'Score of {name} for validation dataset is : {score}')\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(test_vectors.toarray())\n",
    "    else:\n",
    "        Z = clf.predict_proba(test_vectors.toarray())\n",
    "    print(f'The predicted probability of {name} for test data is: {Z}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM\n",
      "Score of Linear SVM for validation dataset is : 0.20027816411682892\n",
      "The predicted probability of Linear SVM for test data is: [[ 2.21744004  0.9044473  -0.19861819]\n",
      " [ 2.21422078  0.94557076 -0.20529168]\n",
      " [ 2.21823035  1.00816531 -0.21921983]\n",
      " ...\n",
      " [ 2.21713837  0.9940768  -0.21640096]\n",
      " [ 2.21708749  0.96015678 -0.21131051]\n",
      " [ 2.21581201  0.97813682 -0.21282982]]\n",
      "Decision Tree\n",
      "Score of Decision Tree for validation dataset is : 0.12934631432545202\n",
      "The predicted probability of Decision Tree for test data is: [[0.         0.         1.        ]\n",
      " [0.30697928 0.30261723 0.39040349]\n",
      " [0.30697928 0.30261723 0.39040349]\n",
      " ...\n",
      " [0.43785031 0.3316848  0.23046489]\n",
      " [0.30697928 0.30261723 0.39040349]\n",
      " [0.30697928 0.30261723 0.39040349]]\n",
      "Random Forest\n",
      "Score of Random Forest for validation dataset is : 0.20027816411682892\n",
      "The predicted probability of Random Forest for test data is: [[0.41846435 0.29812429 0.28341136]\n",
      " [0.41846435 0.29812429 0.28341136]\n",
      " [0.41846435 0.29812429 0.28341136]\n",
      " ...\n",
      " [0.41846435 0.29812429 0.28341136]\n",
      " [0.41846435 0.29812429 0.28341136]\n",
      " [0.41846435 0.29812429 0.28341136]]\n",
      "Neural Net\n",
      "Score of Neural Net for validation dataset is : 0.18915159944367177\n",
      "The predicted probability of Neural Net for test data is: [[0.43939093 0.19081883 0.36979024]\n",
      " [0.31484463 0.27032374 0.41483163]\n",
      " [0.40810051 0.38561487 0.20628462]\n",
      " ...\n",
      " [0.3724276  0.345049   0.2825234 ]\n",
      " [0.33302337 0.30698649 0.35999015]\n",
      " [0.32555816 0.32535853 0.34908331]]\n",
      "AdaBoost\n",
      "Score of AdaBoost for validation dataset is : 0.16828929068150209\n",
      "The predicted probability of AdaBoost for test data is: [[ 4.55405457e-01 -9.33250155e-01  4.77844697e-01]\n",
      " [-1.70776248e-02 -1.34230640e-02  3.05006889e-02]\n",
      " [ 2.09197243e-03 -1.76722098e-03 -3.24751448e-04]\n",
      " ...\n",
      " [ 4.88945095e-03  4.80327397e-03 -9.69272492e-03]\n",
      " [-9.71584320e-03  2.02755903e-03  7.68828417e-03]\n",
      " [-9.71584320e-03  2.02755903e-03  7.68828417e-03]]\n",
      "Naive Bayes\n",
      "Score of Naive Bayes for validation dataset is : 0.2267037552155772\n",
      "The predicted probability of Naive Bayes for test data is: [[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Linear SVM\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\"]\n",
    "classifiers = [\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    ]\n",
    "\n",
    "\n",
    "for name, clf14 in zip(names, classifiers):\n",
    "    print(name)\n",
    "    clf14.fit(train_vectors14.toarray(), data_train_cleaned14['label'])\n",
    "    score14 = clf14.score(validate_vectors14.toarray(), data_validate_cleaned14['label'])\n",
    "    print(f'Score of {name} for validation dataset is : {score14}')\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    if hasattr(clf14, \"decision_function\"):\n",
    "        Z14 = clf14.decision_function(test_vectors14.toarray())\n",
    "    else:\n",
    "        Z14 = clf14.predict_proba(test_vectors14.toarray())\n",
    "    print(f'The predicted probability of {name} for test data is: {Z14}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
